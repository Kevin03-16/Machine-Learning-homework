clear all,close all
clc

n=2;
N=1000;
K=10;
mu=[0;0];
Sigma=eye(2);
p=[0.35, 0.65];
label=rand(1,N)>=p(1);
y=zeros(1,N);
y(1,find(label==1))=1;
y(1,find(label==0))=-1;
Nc=[length(find(y==-1)),length(find(y==1))]
x(:,y==-1)=mvnrnd(mu,Sigma,Nc(1))';
r=2+rand(1,Nc(2));
theta=-pi+2*pi*rand(1,Nc(2));
x(:,y==1)=[r.*cos(theta);r.*sin(theta)];
%generate data
plot(x(1,y==-1),x(2,y==-1),'ro'),hold on
plot(x(1,y==1),x(2,y==1),'bx'),axis equal

% Divide the data set into K approximately-equal-sized partitions
dummy = ceil(linspace(0,N,K+1));
for k = 1:K
    indPartitionLimits(k,:) = [dummy(k)+1,dummy(k+1)];
end
% Allocate space
Dtrain = zeros(K,N); Dvalidate = zeros(K,N); 
AveragepTrainingError = zeros(1,N); AveragepValidateError = zeros(1,N);
%for Linear svm

for i=5
    C=10^(i-5);
    
    % K-fold cross validation
    for k = 1:K
        indValidate = [indPartitionLimits(k,1):indPartitionLimits(k,2)];
        DValidate = x(:,indValidate); % Using folk k as validation set
        if k == 1
            indTrain = [indPartitionLimits(k,2)+1:N];
        elseif k == K
            indTrain = [1:indPartitionLimits(k,1)-1];
        else
            indTrain = [1:indPartitionLimits(k-1,2),indPartitionLimits(k+1,1):N];
        end
        DTrain = x(:,indTrain); % using all other folds as training set
        Dvalidate=x(:,indValidate);
        Ntrain = length(indTrain); Nvalidate = length(indValidate);
        lTrain=y(indTrain);
        lValidate=y(indValidate);
        % Train model parameters
        trainedSVM = fitcsvm(DTrain',lTrain,'BoxConstraint',C,'KernelFunction','linear');
        dValidate= trainedSVM.predict(Dvalidate')';% Labels of training data using the trained SVM
        figure(k+1)
        indINCORRECT_Validate = find(lValidate.*dValidate == -1); % Find training samples that are incorrectly classified by the trained SVM
        indCORRECT_Validate = find(lValidate.*dValidate == 1); % Find training samples that are correctly classified by the trained SVM
        pValidateError = length(indINCORRECT_Validate)/Nvalidate; % Empirically estimate of training error probability
        plot(x(1,indINCORRECT_Validate),x(2,indINCORRECT_Validate),'r.'), hold on,
        plot(x(1,indINCORRECT_Validate),x(2,indINCORRECT_Validate),'g.'), axis equal,
        pause(3)
        
        %indINCORRECT_Validate = find(lValidate.*dValidate == -1); % Find training samples that are incorrectly classified by the trained SVM
        %indCORRECT_Validate = find(lValidate.*dValidate == 1); % Find training samples that are correctly classified by the trained SVM
        %pValidateError = length(indINCORRECT_Validate)/Nvalidate; % Empirically estimate of training error probability
    end   
    %AveragepTrainingError(1,i)  = mean(pTrainingError,1); % average training MSE over folds
    %AveragepValidateError(1,i)  = mean(pValidateError,1); % average validation MSE over folds
end
%figure(2), clf,
%semilogy(AveragepTrainingError,'.b'); hold on; semilogy(AveragepValidateError,'rx');
%xlabel('Model Polynomial Order'); ylabel(strcat('MSE estimate with ',num2str(K),'-fold cross-validation'));
%legend('Training MSE','Validation MSE');

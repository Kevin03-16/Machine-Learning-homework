clear all, close all
clc

for i=4
    delta = 1e-5; % tolerance for EM stopping criterion
    regWeight = 1e-10; % regularization parameter for covariance estimates
    N=10^i;%generate samples 10,100,1000,10000
    K=10;%10-FOLDER
    
    %generate samples from a 4-component GMM
    alpha_true=[0.1,0.25,0.25,0.4];
    mu_true=[[-5;-5],[5;-5],[5;5],[-5;5]];
    Sigma_true(:,:,1)=[2 1;1 2];
    Sigma_true(:,:,2)=[3 1;1 4];
    Sigma_true(:,:,3)=[15 1;1 6];
    Sigma_true(:,:,4)=[2 1;1 15];
    x=randGMM(N,alpha_true,mu_true,Sigma_true);
    figure(i)
    plot(x(1,:),x(2,:),'rx')
    xlabel('x1'),ylabel('x2'),axis equal
    title('Generated data by true GMM')
    [d,Num]=size(mu_true);%determine dimensionality of samples and the number of GMM components
    
    %Divide the data set into K approximately-equal-sized partitions
    dummy = ceil(linspace(0,N,K+1));%ceil Round towards plus infinity.
    for k = 1:K
        indPartitionLimits(k,:) = [dummy(k)+1,dummy(k+1)];%eg.[dummy(1)+1,dummy(2)]
    end
    % Allocate space
    MLEtrain = zeros(K,N); MLEvalidate = zeros(K,N); 
    AverageMLEtrain = zeros(1,N); AverageMLEvalidate = zeros(1,N);
    %train model from 1 to 6
 
    for M=1:Num+2%#order
        alpha = ones(1,M)/M;
        shuffledIndices = randperm(N);
        mu = x(:,shuffledIndices(1:M)); % pick M random samples as initial mean estimates
        [~,assignedCentroidLabels] = min(pdist2(mu',x'),[],1); % assign each sample to the nearest mean
        for m = 1:M % use sample covariances of initial assignments as initial covariance estimates
            Sigma(:,:,m) = cov(x(:,find(assignedCentroidLabels==m))') + regWeight*eye(d,d);
        end
% K-fold cross validation 

        for k = 1:K
            %set train data and validate data
            indValidate = [indPartitionLimits(k,1):indPartitionLimits(k,2)];%choose one dataspace to be validation set
            Dvalidate=x(:,indValidate);
            if k == 1
                indTrain = [indPartitionLimits(k,2)+1:N];
            elseif k == K
                indTrain = [1:indPartitionLimits(k,1)-1];
            else
                indTrain = [[1:indPartitionLimits(k-1,2)],[indPartitionLimits(k+1,1):N]];
            end
            DTrain=x(:,indTrain);
            NTrain = length(indTrain); Nvalidate = length(indValidate);
            
            % Train model parameters
            t=0;
            Converage=0;
            while ~Converage
                for m=1:M
                    temp(m,:)=repmat(alpha(m),1,NTrain).*evalGaussian(DTrain,mu(:,m),Sigma(:,:,m));%temp is mxN
                end
                plgivenx = temp./sum(temp,1);
                alphaNew = mean(plgivenx,2);
                w = plgivenx./repmat(sum(plgivenx,2),1,NTrain);%w is mxN
                muNew = DTrain*w';%is 2xm
                for m = 1:M
                    v = DTrain-repmat(muNew(:,m),1,NTrain);%v is 2xN
                    u = repmat(w(m,:),d,1).*v;%2xN
                    SigmaNew(:,:,m) = u*v' + regWeight*eye(d,d);% adding a small regularization term
                end
                Dalpha = sum(abs(alphaNew-alpha));
                Dmu = sum(sum(abs(muNew-mu)));
                DSigma = sum(sum(abs(abs(SigmaNew-Sigma))));
                Converged = ((Dalpha+Dmu+DSigma)<delta); % Check if converged
                alpha = alphaNew; mu = muNew; Sigma = SigmaNew;
                t = t+1;
                if t>20
                    break
                end
%                 try
%                     displayProgress(M,N,t,DTrain,alpha,mu,Sigma);
%                 catch
%                     Warning('Matrix is singular, close to singular or badly scaled. Results may be inaccurate. RCOND = NaN.')
%                     break
%                 end
                MLETrain(k,M)=sum(log(evalGMM(DTrain,alpha,mu,Sigma)));
                MLEValidate(k,M)=sum(log(evalGMM(Dvalidate,alpha,mu,Sigma)));
            end
        end
        AverageMLEtrain(1,M) = mean(MLETrain(:,M)); % average training MLE over folds
        AverageMLEvalidate(1,M) = mean(MLEValidate(:,M)); % average validation MLE over folds
    end
    figure(i+1)
    semilogy(AverageMLEtrain,'.b'); hold on; semilogy(AverageMLEvalidate,'rx');
    xlabel('Model component number'); ylabel(strcat('MLE estimate with ',num2str(K),'-fold cross-validation'));
    title(['MLE estimate for different components with ',num2str(N),' samples'])
    legend('Training MLE','Validation MLE');
end


    
    
    
function displayProgress(M,N,t,x,alpha,mu,Sigma)
figure(M)
if size(x,1)==2
    subplot(1,2,1), 
    cla,
    plot(x(1,:),x(2,:),'b.'); 
    xlabel('x_1'), ylabel('x_2'), title(['Data and Estimated GMM Contours for ', num2str(M), ' components with ', num2str(N), ' samples']),
    axis equal, hold on;
    rangex1 = [min(x(1,:)),max(x(1,:))];
    rangex2 = [min(x(2,:)),max(x(2,:))];
    [x1Grid,x2Grid,zGMM] = contourGMM(alpha,mu,Sigma,rangex1,rangex2);
    contour(x1Grid,x2Grid,zGMM); axis equal, 
    subplot(1,2,2), 
end
logLikelihood = sum(log(evalGMM(x,alpha,mu,Sigma)));
plot(t,logLikelihood,'b.'); hold on,
xlabel('Iteration Index'), ylabel('Log-Likelihood of Data'),
drawnow; pause(0.1);
end

function [x1Grid,x2Grid,zGMM] = contourGMM(alpha,mu,Sigma,rangex1,rangex2)
x1Grid = linspace(floor(rangex1(1)),ceil(rangex1(2)),101);
x2Grid = linspace(floor(rangex2(1)),ceil(rangex2(2)),91);
[h,v] = meshgrid(x1Grid,x2Grid);
GMM = evalGMM([h(:)';v(:)'],alpha, mu, Sigma);
zGMM = reshape(GMM,91,101);
%figure(1), contour(horizontalGrid,verticalGrid,discriminantScoreGrid,[minDSGV*[0.9,0.6,0.3],0,[0.3,0.6,0.9]*maxDSGV]); % plot equilevel contours of the discriminant function 
end


function x=randGMM(N,alpha,mu,Sigma)
d=size(mu,1);%dimensionality of samples
cum_alpha=[0,cumsum(alpha)];
u=rand(1,N);%randomly generate N numbers
x=zeros(d,N);labels=zeros(1,N);
for m=1:length(alpha)%iteration for the number of component
    ind=find(cum_alpha(m)<u & u<=cum_alpha(m+1));
    x(:,ind)=randGaussian(length(ind),mu(:,m),Sigma(:,:,m));%generate samples for each component
end
end

%generate N samples from a Gaussian pdf with mean mu and covariance Sigma
function x=randGaussian(N,mu,Sigma)
n=length(mu);
z=randn(n,N);
A=Sigma^(1/2);
x=A*z+repmat(mu,1,N);
end

function gmm = evalGMM(x,alpha,mu,Sigma)
gmm = zeros(1,size(x,2));
for m = 1:length(alpha) % evaluate the GMM on the grid
    gmm = gmm + alpha(m)*evalGaussian(x,mu(:,m),Sigma(:,:,m));
end
end
%%%

function g = evalGaussian(x,mu,Sigma)
% Evaluates the Gaussian pdf N(mu,Sigma) at each coumn of X
[n,N] = size(x);
C = (2*pi)^(-n/2) * det(Sigma)^(-1/2);
E = -0.5*sum((x-repmat(mu,1,N)).*(inv(Sigma)*(x-repmat(mu,1,N))),1);
g = C*exp(E);
end


clear all,close all
clc

%generate data for training and test
n=2;N=1000;NTest=1000;
K=10;
mu=[0;0];Sigma=eye(2);
p=[0.35, 0.65];
label=rand(1,N)>=p(1);l=2*(label-0.5);
labelTest=rand(1,NTest)>=p(1);lTest=2*(labelTest-0.5);
Nc=[length(find(l==-1)),length(find(l==1))];%number of samples for each label
NcTest=[length(find(lTest==-1)),length(find(lTest==1))];
x(:,l==-1)=mvnrnd(mu,Sigma,Nc(1))';
xTest(:,lTest==-1)=mvnrnd(mu,Sigma,NcTest(1))';
r=2+rand(1,Nc(2));
rTest=2+rand(1,NcTest(2));
theta=-pi+2*pi*rand(1,Nc(2));thetaTest=-pi+2*pi*rand(1,NcTest(2));
x(:,l==1)=[r.*cos(theta);r.*sin(theta)];xTest(:,lTest==1)=[rTest.*cos(thetaTest);rTest.*sin(thetaTest)];
%generate data
figure(1)
plot(x(1,l==-1),x(2,l==-1),'go'),hold on
plot(x(1,l==1),x(2,l==1),'bo'),axis equal
legend('Class -','Class +')
title('True data distribution')
figure(2)
plot(xTest(1,lTest==-1),xTest(2,lTest==-1),'go'),hold on
plot(xTest(1,lTest==1),xTest(2,lTest==1),'bo'),axis equal
legend('Class -','Class +')
title('Test data distribution')
%==============================================================================================================

dummy = ceil(linspace(0,N,K+1));
for k = 1:K, indPartitionLimits(k,:) = [dummy(k)+1,dummy(k+1)]; end,


% % Train a Linear kernel SVM with cross-validation
% % to select hyperparameters that minimize probability 
% % of error (i.e. maximize accuracy; 0-1 loss scenario)

% 
CList = 10.^linspace(-4,4,9);%choose C from this range
for CCounter = 1:length(CList)
    [CCounter,length(CList)],
    C = CList(CCounter);
    %K folder
    for k = 1:K
        indValidate = [indPartitionLimits(k,1):indPartitionLimits(k,2)];
        xValidate = x(:,indValidate); % Using folk k as validation set
        lValidate = l(indValidate);
        if k == 1
            indTrain = [indPartitionLimits(k,2)+1:N];
        elseif k == K
            indTrain = [1:indPartitionLimits(k,1)-1];
        else
            indTrain = [1:indPartitionLimits(k-1,2),indPartitionLimits(k+1,1):N];
        end
        % using all other folds as training set
        xTrain = x(:,indTrain); lTrain = l(indTrain);
        
        %train svm model
        SVMk = fitcsvm(xTrain',lTrain,'BoxConstraint',C,'KernelFunction','linear');
        dValidate = SVMk.predict(xValidate')'; % Labels of validation data using the trained SVM
        indINCORRECT = find(lValidate.*dValidate == -1); 
        NIncorrect(k)=length(indINCORRECT);
    end 
    PIncorrect(CCounter)= sum(NIncorrect)/N; 
end 
figure(3), subplot(1,2,1),
plot(log10(CList),PIncorrect,'.',log10(CList),PIncorrect,'-'),
xlabel('log_{10} C'),ylabel('Probability of Error'),
title('Probability of Error Esimate From Linear-SVM Cross Validation'), %axis equal,
%find the minimum probability of error
[dummy,indi] = min(PIncorrect(:));
fprintf('the minimum probability of error estimate from cross validation is %f',dummy)
[indBestC, ~] = ind2sub(size(PIncorrect),indi);
CBest= CList(indBestC)
%==================================================================================================================

%Use the optimized parameter C to train SVM model(use the whole data)
SVMBest = fitcsvm(x',l','BoxConstraint',CBest,'KernelFunction','linear');
d = SVMBest.predict(x')'; % Labtrained SVM
indINCORRECT = find(l.*d == -1); % Find training samples that are incorrectly classified by the trained SVM
indCORRECT = find(l.*d == 1); % Find training samples that are correctly classified by the trained SVM

figure(3), subplot(1,2,2), 
plot(x(1,indCORRECT),x(2,indCORRECT),'g.'), hold on,
plot(x(1,indINCORRECT),x(2,indINCORRECT),'r.'), axis equal,
title('Training Data (RED: Incorrectly Classified)'),
pTrainingError = length(indINCORRECT)/N, % Empirical estimate of training error probability
Nx = 1001; Ny = 990; xGrid = linspace(-10,10,Nx); yGrid = linspace(-10,10,Ny);
[h,v] = meshgrid(xGrid,yGrid); dGrid = SVMBest.predict([h(:),v(:)]); zGrid = reshape(dGrid,Ny,Nx);
figure(3), subplot(1,2,2), contour(xGrid,yGrid,zGrid,0); xlabel('x1'), ylabel('x2'), axis equal,
%======================================================================================================================

%Test samples
dTest = SVMBest.predict(xTest')'; % Labtrained SVM
indINCORRECT_Test = find(lTest.*dTest == -1); % Find training samples that are incorrectly classified by the trained SVM
indCORRECT_Test = find(lTest.*dTest == 1); % Find training samples that are correctly classified by the trained SVM

figure(4),
plot(x(1,indCORRECT_Test),x(2,indCORRECT_Test),'g.'), hold on,
plot(x(1,indINCORRECT_Test),x(2,indINCORRECT_Test),'r.'), axis equal,
title('Test Data (RED: Incorrectly Classified)'),
pTestError = length(indINCORRECT_Test)/N, % Empirical estimate of training error probability
Nx = 1001; Ny = 990; xGrid = linspace(-10,10,Nx); yGrid = linspace(-10,10,Ny);
[h,v] = meshgrid(xGrid,yGrid); dGrid = SVMBest.predict([h(:),v(:)]); zGrid = reshape(dGrid,Ny,Nx);
contour(xGrid,yGrid,zGrid,0); xlabel('x1'), ylabel('x2'), axis equal,

%=========================================================================================================================


% Train a Gaussian kernel SVM with cross-validation
% to select hyperparameters that minimize probability 
% of error (i.e. maximize accuracy; 0-1 loss scenario)

% parameter for Gaussian Kernel SVM
CList = 10.^linspace(-1,9,11); 
sigmaList = 10.^linspace(-2,3,13);
for sigmaCounter = 1:length(sigmaList)
    [sigmaCounter,length(sigmaList)],
    sigma = sigmaList(sigmaCounter);
    for CCounter = 1:length(CList)
        C = CList(CCounter);
        for k = 1:K
            indValidate = [indPartitionLimits(k,1):indPartitionLimits(k,2)];
            xValidate = x(:,indValidate); % Using folk k as validation set
            lValidate = l(indValidate);
            if k == 1
                indTrain = [indPartitionLimits(k,2)+1:N];
            elseif k == K
                indTrain = [1:indPartitionLimits(k,1)-1];
            else
                indTrain = [1:indPartitionLimits(k-1,2),indPartitionLimits(k+1,1):N];
            end
            % using all other folds as training set
            xTrain = x(:,indTrain); lTrain = l(indTrain);
            SVMk = fitcsvm(xTrain',lTrain,'BoxConstraint',C,'KernelFunction','gaussian','KernelScale',sigma);
            dValidate = SVMk.predict(xValidate')'; % Labels of validation data using the trained SVM
            indINCORRECT = find(lValidate.*dValidate == -1); 
            NIncorrect(k)=length(indINCORRECT);
        end 
        PIncorrect(CCounter,sigmaCounter)= sum(NIncorrect)/N;
    end 
end


figure(5), subplot(1,2,1),
contour(log10(CList),log10(sigmaList),PIncorrect',20); 
xlabel('log_{10} C'), ylabel('log_{10} sigma'),
title('Probability of Error Esitimate from Gaussian-SVM Cross-Validation'), axis equal,

%Choose the parameter when the probalility is maximum
[dummy,indi] = min(PIncorrect(:));
fprintf('the minimum probability of error estimate from cross validation is %f',dummy)
[indBestC, indBestSigma] = ind2sub(size(PIncorrect),indi);
%    [I,J] = ind2sub(SIZ,IND) returns the arrays I and J containing the
%    equivalent row and column subscripts corresponding to the index
%    matrix IND for a matrix of size SIZ. 
CBest= CList(indBestC)
sigmaBest= sigmaList(indBestSigma)
%========================================================================================================================

SVMBest = fitcsvm(x',l','BoxConstraint',CBest,'KernelFunction','gaussian','KernelScale',sigmaBest);
d = SVMBest.predict(x')'; % Labels of training data using the trained SVM

indINCORRECT = find(l.*d == -1); % Find training samples that are incorrectly classified by the trained SVM
indCORRECT = find(l.*d == 1); % Find training samples that are correctly classified by the trained SVM
figure(5), subplot(1,2,2), 
plot(x(1,indCORRECT),x(2,indCORRECT),'g.'), hold on,
plot(x(1,indINCORRECT),x(2,indINCORRECT),'r.'), axis equal,
title('Training Data (RED: Incorrectly Classified)'),
pTrainingError = length(indINCORRECT)/N, % Empirical estimate of training error probability
NTrainingErrir = length(indINCORRECT)
Nx = 1001; Ny = 990; xGrid = linspace(-10,10,Nx); yGrid = linspace(-10,10,Ny);
[h,v] = meshgrid(xGrid,yGrid); dGrid = SVMBest.predict([h(:),v(:)]); zGrid = reshape(dGrid,Ny,Nx);
figure(5), subplot(1,2,2), contour(xGrid,yGrid,zGrid,0); xlabel('x1'), ylabel('x2'), axis equal,

%================================================================================================================================
%Test samples
dTest = SVMBest.predict(xTest')'; % Labels of training data using the trained SVM

indINCORRECT_Test = find(lTest.*dTest== -1); % Find training samples that are incorrectly classified by the trained SVM
indCORRECT_Test = find(lTest.*dTest == 1); % Find training samples that are correctly classified by the trained SVM
figure(6),
plot(xTest(1,indCORRECT_Test),xTest(2,indCORRECT_Test),'g.'), hold on,
plot(xTest(1,indINCORRECT_Test),xTest(2,indINCORRECT_Test),'r.'), axis equal,
title('Test Data (RED: Incorrectly Classified)'),
pTestError = length(indINCORRECT_Test)/N, % Empirical estimate of training error probability
NTestErrir = length(indINCORRECT_Test)
Nx = 1001; Ny = 990; xGrid = linspace(-10,10,Nx); yGrid = linspace(-10,10,Ny);
[h,v] = meshgrid(xGrid,yGrid); dGrid = SVMBest.predict([h(:),v(:)]); zGrid = reshape(dGrid,Ny,Nx);
contour(xGrid,yGrid,zGrid,0); xlabel('x1'), ylabel('x2'), axis equal,

